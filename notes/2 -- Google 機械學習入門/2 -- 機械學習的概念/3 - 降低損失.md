# 降低损失（Reducing Loss）

为了训练模型，我们需要一种可降低模型损失的好方法。迭代方法是一种广泛用于降低损失的方法，而且使用起来简单有效。

## **学习目标**

- 了解如何使用迭代方法来训练模型。

- 全面了解梯度下降法和一些变体，包括：

- - 小批量梯度下降法
  - 随机梯度下降法

- 尝试不同的学习速率。

## 权重初始化

- 对于凸形问题，权重可以从任何位置开始（比如，所有值为 0 的位置）

- - 凸形：想象一个碗的形状（如下图[1]所示）
  - 只有一个最低点

- 预示：不适用于神经网络

- - 非凸形：想象一个蛋托的形状（如下图[2]所示）
  - 有多个最低点
  - 很大程度上取决于初始值


- ​

## 降低损失：迭代方法

[上一单元](深入了解机器学习.md)介绍了损失的概念。在本单元中，您将了解机器学习模型如何以迭代方式降低损失。

迭代学习可能会让您想到“[Hot and Cold](http://www.howcast.com/videos/258352-how-to-play-hot-and-cold/)”这种寻找隐藏物品（如顶针）的儿童游戏。在我们的游戏中，“隐藏的物品”就是最佳模型。刚开始，您会胡乱猜测（“w1 的值为 0。”），等待系统告诉您损失是多少。然后，您再尝试另一种猜测（“w1 的值为 0.5。”），看看损失是多少。哎呀，这次更接近目标了。实际上，如果您以正确方式玩这个游戏，通常会越来越接近目标。这个游戏真正棘手的地方在于尽可能高效地找到最佳模型。

下图显示了机器学习算法用于训练模型的迭代试错过程：

![](https://note.youdao.com/yws/public/resource/e5d22712496f4f6585b7247eac93871c/xmlnote/WEBRESOURCE758f7c3b4d00204c240e760bfe299b90/50128)

**图 1. 用于训练模型的迭代方法。**

我们将在整个机器学习速成课程中使用相同的迭代方法详细说明各种复杂情况，尤其是处于暴风雨中的蓝云区域。迭代策略在机器学习中的应用非常普遍，这主要是因为它们可以很好地扩展到大型数据集。

“模型”部分将一个或多个特征作为输入，然后返回一个预测 (y') 作为输出。为了进行简化，不妨考虑一种采用一个特征并返回一个预测的模型：

y′=b+w1x1

我们应该为 b 和 w1 设置哪些初始值？对于线性回归问题，事实证明初始值并不重要。我们可以随机选择值，不过我们还是选择采用以下这些无关紧要的值：

- b = 0
- w1 = 0

假设第一个特征值是 10。将该特征值代入预测函数会得到以下结果：

```
  y' = 0 + 0(10)
  y' = 0

```

图中的“计算损失”部分是模型将要使用的[损失函数](深入了解机器学习.md)。假设我们使用平方损失函数。损失函数将采用两个输入值：

- **y'：模型对特征 x 的预测**
- **y：特征 x 对应的正确标签。**

最后，我们来看图的“计算参数更新”部分。机器学习系统就是在此部分检查损失函数的值，并为 b 和 w1 生成新值。现在，假设这个神秘的绿色框会产生新值（绿色框如图1所示），然后机器学习系统将根据所有标签重新评估所有特征，为损失函数生成一个新值，而该值又产生新的参数值。这种学习过程会持续迭代，直到该算法发现损失可能最低的模型参数。通常，您可以不断迭代，直到总体损失不再变化或至少变化极其缓慢为止。这时候，我们可以说该模型已**收敛**。

**要点：**在训练机器学习模型时，首先对权重和偏差进行初始猜测，然后反复调整这些猜测，直到获得损失可能最低的权重和偏差为止。

**关键字词**

- [收敛](https://developers.google.com/machine-learning/crash-course/glossary#convergence)：通俗来说，收敛通常是指在训练期间达到的一种状态，即经过一定次数的迭代之后，训练[**损失**](https://developers.google.com/machine-learning/crash-course/glossary#loss)和验证损失在每次迭代中的变化都非常小或根本没有变化。也就是说，如果采用当前数据进行额外的训练将无法改进模型，模型即达到收敛状态。在深度学习中，损失值有时会在最终下降之前的多次迭代中保持不变或几乎保持不变，暂时形成收敛的假象。

  另请参阅[**早停法**](https://developers.google.com/machine-learning/crash-course/glossary#early_stopping)。

  另请参阅 Boyd 和 Vandenberghe 合著的 [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)（《凸优化》）。

- [损失](https://developers.google.com/machine-learning/glossary#loss)：一种衡量指标，用于衡量模型的[**预测**](https://developers.google.com/machine-learning/glossary/#prediction)偏离其[**标签**](https://developers.google.com/machine-learning/glossary/#label)的程度。或者更悲观地说是衡量模型有多差。要确定此值，模型必须定义损失函数。例如，线性回归模型通常将[**均方误差**](https://developers.google.com/machine-learning/glossary/#MSE)用于损失函数，而逻辑回归模型则使用[**对数损失函数**](https://developers.google.com/machine-learning/glossary/#Log_Loss)。

- [训练](https://developers.google.com/machine-learning/glossary#training)：确定构成模型的理想[**参数**](https://developers.google.com/machine-learning/glossary/#parameter)的过程。

## 降低损失（Reducing Loss）：梯度下降法GD

梯度下降法：一种通过计算并且减小梯度将[**损失**](https://developers.google.com/machine-learning/crash-course/glossary#loss)降至最低的技术，它以训练数据为条件，来计算损失相对于模型参数的梯度。通俗来说，梯度下降法以迭代方式调整参数，逐渐找到[**权重**](https://developers.google.com/machine-learning/crash-course/glossary#weight)和偏差的最佳组合，从而将损失降至最低。

迭代方法图（[图 1](https://developers.google.com/machine-learning/crash-course/reducing-loss/an-iterative-approach#ml-block-diagram)）包含一个标题为“计算参数更新”的华而不实的绿框。现在，我们将用更实质的方法代替这种华而不实的算法。

假设我们有时间和计算资源来计算 w1 的所有可能值的损失。对于我们一直在研究的回归问题，所产生的损失与 w1 的图形始终是凸形。换言之，图形始终是碗状图，如下所示：

![](https://note.youdao.com/yws/public/resource/e5d22712496f4f6585b7247eac93871c/xmlnote/WEBRESOURCE1f5a79d11e937b6707940a5d97721df9/50130)

**图 2. 回归问题产生的损失与权重图为凸形。**

 

凸形问题只有一个最低点；即只存在一个斜率正好为 0 的位置。这个最小值就是损失函数收敛之处。

通过计算整个数据集中 w1 每个可能值的损失函数来找到收敛点这种方法效率太低。我们来研究一种更好的机制，这种机制在机器学习领域非常热门，称为**梯度下降法**。

梯度下降法的第一个阶段是为 w1 选择一个起始值（起点）。起点并不重要；因此很多算法就直接将 w1 设为 0 或随机选择一个值。下图显示的是我们选择了一个稍大于 0 的起点：

![](https://note.youdao.com/yws/public/resource/e5d22712496f4f6585b7247eac93871c/xmlnote/WEBRESOURCE5c9d31b80b197dc362dc8dd7be0fb244/50132)

**图 3. 梯度下降法的起点。**

然后，梯度下降法算法会计算损失曲线在起点处的梯度。简而言之，**梯度**是偏导数的矢量；它可以让您了解哪个方向距离目标“更近”或“更远”。请注意，损失相对于单个权重的梯度（如图 3 所示）就等于导数。

▸详细了解偏导数和梯度。

请注意，梯度是一个矢量，因此具有以下两个特征：

- 方向
- 大小

梯度始终指向损失函数中增长最为快速的方向。**梯度下降法算法会沿着负梯度的方向走一步，以便尽快降低损失。**

![](https://note.youdao.com/yws/public/resource/e5d22712496f4f6585b7247eac93871c/xmlnote/WEBRESOURCE047d40ac9f1d56c82390f4673874b25a/50134)

**图 4. 梯度下降法依赖于负梯度。**

为了确定损失函数曲线上的下一个点，梯度下降法算法会将梯度大小的一部分与起点相加（权重值的变化方式应该是与梯度方向相反，即沿着负梯度的方向，而当前点梯度为负数，所以负梯度方向即为正，权重w应该是增加的），如下图所示：

![](https://note.youdao.com/yws/public/resource/e5d22712496f4f6585b7247eac93871c/xmlnote/WEBRESOURCE350e19b104deb498e1fa3af0f8a094b2/50136)

**图 5. 一个梯度步长将我们移动到损失曲线上的下一个点。**

然后，梯度下降法会重复此过程，逐渐接近最低点。

**关键字词**

- [梯度下降法](https://developers.google.com/machine-learning/crash-course/glossary#gradient_descent)：一种通过计算并且减小梯度将[**损失**](https://developers.google.com/machine-learning/crash-course/glossary#loss)降至最低的技术，它以训练数据为条件，来计算损失相对于模型参数的梯度。通俗来说，梯度下降法以迭代方式调整参数，逐渐找到[**权重**](https://developers.google.com/machine-learning/crash-course/glossary#weight)和偏差的最佳组合，从而将损失降至最低。
- [步长](https://developers.google.com/machine-learning/crash-course/glossary#step) ：是[**学习速率**](https://developers.google.com/machine-learning/crash-course/glossary#learning_rate)的同义词。

## 降低损失 (Reducing Loss)：学习速率LR

正如之前所述，梯度矢量具有方向和大小。梯度下降法算法用梯度乘以一个称为**学习速率（learning rate）**（有时也称为**步长**）的标量，以确定下一个点的位置。例如，如果梯度大小为 2.5，学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的位置作为下一个点。

**超参数**是编程人员在机器学习算法中用于调整的旋钮。大多数机器学习编程人员会花费相当多的时间来调整学习速率（简称：调参）。**如果您选择的学习速率过小，就会花费太长的学习时间。**为什么学习速率小，花费的时间就长呢？因为学习速率小，梯度下降的值就小，使得权重值更新的变化小，因此收敛就会慢，时间耗时长。

![](https://note.youdao.com/yws/public/resource/e5d22712496f4f6585b7247eac93871c/xmlnote/WEBRESOURCE72c2b4ab1bbfeb5b6c40d0c5ef6bcd88/50138)

**图 6. 学习速率过小。**

相反，如果您指定的学习速率过大，下一个点将永远在 U 形曲线的底部随意弹跳，就好像量子力学实验出现了严重错误一样：

![](https://note.youdao.com/yws/public/resource/e5d22712496f4f6585b7247eac93871c/xmlnote/WEBRESOURCEbd247aba87081c0e6b75737d57939644/50140)

**图 7. 学习速率过大。**

每个回归问题都存在一个[金发姑娘](https://wikipedia.org/wiki/Goldilocks_principle)学习速率。“金发姑娘”值与损失函数的平坦程度相关。如果您知道损失函数的梯度较小，则可以放心地试着采用较大的学习速率，以补偿较小的梯度并获得更大的步长。

![](https://note.youdao.com/yws/public/resource/e5d22712496f4f6585b7247eac93871c/xmlnote/WEBRESOURCEdc8d67dea0f640d129a223cdf88aef88/50142)

**图 8. 学习速率恰恰好。**

**关键字词**

| [超参数](https://developers.google.com/machine-learning/crash-course/glossary#hyperparameter) | [学习速率](https://developers.google.com/machine-learning/crash-course/glossary#learning_rate) |
| ---------------------------------------- | ---------------------------------------- |
| [步长](https://developers.google.com/machine-learning/crash-course/glossary#step_size) |                                          |

- **超参数（hyperparameter）：**在模型训练的连续过程中，您调节的“旋钮”。例如，[**学习速率**](https://developers.google.com/machine-learning/crash-course/glossary#learning_rate)就是一种超参数。与[**参数**](https://developers.google.com/machine-learning/crash-course/glossary#parameter)相对。
- **学习速率（learning rate）：**在训练模型时用于梯度下降的一个变量。在每次迭代期间，[**梯度下降法**](https://developers.google.com/machine-learning/crash-course/glossary#gradient_descent)都会将学习速率与梯度相乘。得出的乘积称为**梯度步长**。学习速率是一个重要的[**超参数**](https://developers.google.com/machine-learning/crash-course/glossary#hyperparameter)。
- **步长（step size）**：是[**学习速率**](https://developers.google.com/machine-learning/crash-course/glossary#learning_rate)的同义词。

## 优化学习速率

[（练习题）举个栗子](https://developers.google.com/machine-learning/crash-course/fitter/graph)：尝试不同的学习速率，看看不同的学习速率对到达损失曲线最低点所需的步数有何影响。请尝试进行图表下方的练习。

![](https://note.youdao.com/yws/public/resource/e5d22712496f4f6585b7247eac93871c/xmlnote/WEBRESOURCE58fdaa463e8a12bd7da2ed34e4b6d079/50144)



当学习速率为0.7时，需要执行10步，模型的损失才会降到最低，如下图所示：

![](https://note.youdao.com/yws/public/resource/e5d22712496f4f6585b7247eac93871c/xmlnote/WEBRESOURCEed765523fe8b33891498cbab52a96803/50122)



但增大学习速率，比如设为1.80时，只需要执行3步，模型的损失才会降到最低，如下图所示：

![](https://note.youdao.com/yws/public/resource/e5d22712496f4f6585b7247eac93871c/xmlnote/WEBRESOURCE6d648e9f9293d341a8beb813fed5fda2/50124)

但要注意，学习率不能设置太大，一般在0~1之间。比如将学习速率设为4后，每次迭代后，损失会越来越大，永远不会收敛。

## 降低损失 (Reducing Loss)：随机梯度下降法SGD

在梯度下降法中，**批量（batch）**指的是用于在单次迭代中计算梯度的**样本总数**。到目前为止，我们一直假定批量是指整个数据集。就 Google 的规模而言，数据集通常包含数十亿甚至数千亿个样本。此外，Google 数据集通常包含海量特征。因此，一个批量可能相当巨大。如果是超大批量，则单次迭代就可能要花费很长时间进行计算。

包含随机抽样样本的大型数据集可能包含冗余数据。实际上，批量越大，出现冗余的可能性就越高。一些冗余可能有助于消除杂乱的梯度，但超大批量所具备的预测价值往往并不比大型批量高。

如果我们可以通过更少的计算量得出正确的平均梯度，会怎么样？通过从我们的数据集中随机选择样本，我们可以通过小得多的数据集估算（尽管过程非常杂乱）出较大的平均值。 **随机梯度下降法** (**SGD**) 将这种想法运用到极致，它每次迭代只使用**一个样本（批量大小为 1）**。如果进行足够的迭代，SGD 也可以发挥作用，但过程会非常杂乱。“随机”这一术语表示构成各个批量的一个样本都是随机选择的。

**小批量随机梯度下降法**（**Mini-Batch SGD**）是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。

为了简化说明，我们只针对单个特征重点介绍了梯度下降法。请放心，梯度下降法也适用于包含多个特征的特征集。

**关键字词**

| [批量](https://developers.google.com/machine-learning/crash-course/glossary#batch) | [批量大小](https://developers.google.com/machine-learning/crash-course/glossary#batch_size) |
| ---------------------------------------- | ---------------------------------------- |
| [小批量](https://developers.google.com/machine-learning/crash-course/glossary#mini-batch) | [随机梯度下降法](https://developers.google.com/machine-learning/crash-course/glossary#stochastic_gradient_descent_(SGD)) |

- **批量（batch）：**[**模型训练**](https://developers.google.com/machine-learning/crash-course/glossary#model_training)的一次[**迭代**](https://developers.google.com/machine-learning/crash-course/glossary#iteration)（即一次[**梯度**](https://developers.google.com/machine-learning/crash-course/glossary#gradient)更新）中使用的样本集。另请参阅[**批次规模**](https://developers.google.com/machine-learning/crash-course/glossary#batch_size)。
- **批量大小（batch size）：**一个[**批次**](https://developers.google.com/machine-learning/crash-course/glossary#batch)中的样本数。例如，[**SGD**](https://developers.google.com/machine-learning/crash-course/glossary#SGD) 的批次规模为 1，而[**小批次**](https://developers.google.com/machine-learning/crash-course/glossary#mini-batch)的规模通常介于 10 到 1000 之间。批次规模在训练和推断期间通常是固定的；不过，TensorFlow 允许使用动态批次规模。
- **小批量（mini-batch）：**从训练或推断过程的一次迭代中一起运行的整批[**样本**](https://developers.google.com/machine-learning/crash-course/glossary#example)内随机选择的一小部分。小批次的[**规模**](https://developers.google.com/machine-learning/crash-course/glossary#batch_size)通常介于 10 到 1000 之间。与基于完整的训练数据计算损失相比，基于小批次数据计算损失要高效得多。
- **随机梯度下降法（SGD，stochastic gradient descent）：**批次规模为 1 的一种[**梯度下降法**](https://developers.google.com/machine-learning/crash-course/glossary#gradient_descent)。换句话说，SGD 依赖于从数据集中随机均匀选择的单个样本来计算每步的梯度估算值。

## 降低损失 (Reducing Loss)：Playground 练习

### 学习速率和收敛

这是一系列 Playground 练习中的第一个练习。 [Playground](http://playground.tensorflow.org/) 是专为本课程开发的教程，旨在讲解机器学习原理。

每个 Playground 练习都会生成一个数据集。此数据集的标签具有两个可能值。您可以将这两个可能值设想成垃圾邮件与非垃圾邮件，或者设想成健康的树与生病的树。大部分练习的目标是调整各种超参数，以构建可成功划分（分开或区分）一个标签值和另一个标签值的模型。请注意，大部分数据集都包含一定数量的杂乱样本，导致无法成功划分每个样本。

每个 Playground 练习都会显示模型当前状态的直观图示。例如，以下就是一个模型的直观图示：

![模型直观图示](https://note.youdao.com/yws/public/resource/e5d22712496f4f6585b7247eac93871c/xmlnote/WEBRESOURCE661bcd930cf14dc435e428ab4400c629/50149)

请注意以下关于模型直观图示的说明：

- 每个蓝点表示一类数据的一个样本（例如，一棵健康的树）。
- 每个橙点表示另一类数据的一个样本（例如，一棵生病的树）。
- 背景颜色表示该模型对于应该在何处找到相应颜色样本的预测。某个蓝点周围显示蓝色背景表示该模型正确地预测了该样本。相反，某个蓝点周围显示橙色背景则表示该模型错误地预测了该样本。
- 背景的蓝色和橙色部分色调会有深浅之分。例如，直观图示的左侧是纯蓝色，但在直观图示的中心颜色则逐渐淡化为白色。您可以将颜色强度视为表明该模型对其猜测结果的自信程度。因此，纯蓝色表示该模型对其猜测结果非常自信，而浅蓝色则表示该模型的自信程度稍低。（图中所示的模型直观图示在预测方面的表现非常糟糕。）

可以通过直观图示来判断模型的进展。（“非常棒 - 大多数蓝点都有蓝色背景”或者“糟糕！蓝点有橙色背景。”）除了颜色之外，Playground 还会以数字形式显示模型的当前损失。（“糟糕！损失正在上升，而不是下降。”）

此练习的界面提供了 3 个按钮：

| 图标                                       | 名称               | 用途                                       |
| ---------------------------------------- | ---------------- | ---------------------------------------- |
| ![“重置”按钮。](https://note.youdao.com/yws/public/resource/e5d22712496f4f6585b7247eac93871c/xmlnote/WEBRESOURCE50a39389d1680ef036e624970ac304f9/50151) | Reset（重置）        | 将 `Iterations` 重置为 0。重置该模型已学习的所有权重。      |
| ![“步”按钮。](https://note.youdao.com/yws/public/resource/e5d22712496f4f6585b7247eac93871c/xmlnote/WEBRESOURCEe3527b6382833da31b0a68cf9ad22081/50153) | Step（步）          | 展开一次新的迭代。对于每次迭代，模型都会发生变化，有时是细微变化，有时是巨大变化。 |
| ![“重新生成”按钮。](https://note.youdao.com/yws/public/resource/e5d22712496f4f6585b7247eac93871c/xmlnote/WEBRESOURCEa65f08cc8c3958d9eaf37cba00b687b8/50155) | Regenerate（重新生成） | 生成一个新数据集。不会重置 `Iterations`。              |

在这第一个 Playground 练习中，您将通过执行以下两个任务来尝试不同的学习速率。

**任务 1**：注意 Playgroud 右上角的**学习速率**菜单。指定学习速率为 3，这个值非常高。通过点击“步”按钮 10 或 20 次，观察这种较高的学习速率会如何影响您的模型。在早期的每次迭代之后，请注意模型的直观图示如何急剧变化。模型似乎已收敛后，您甚至可能看到出现不稳定的情况。另请注意从 x1 和 x2 到模型直观图示之间的线。这些线的权重表示模型中相应特征的权重。也就是说，线越粗，权重越高。

- 学习率为3，点击10步后，loss为0.257，如下图所示：

![](https://note.youdao.com/yws/public/resource/e5d22712496f4f6585b7247eac93871c/xmlnote/WEBRESOURCEa1ff75a01f92f80468035086229e2d4e/50157)



学习率为3，点击20步后，loss为0.255，如下图所示：

![](https://note.youdao.com/yws/public/resource/e5d22712496f4f6585b7247eac93871c/xmlnote/WEBRESOURCE7a73f6d11b55a83c5f40ed446851db9e/50159)



根据loss的变化曲线可知，在前2、3次迭代时，loss是逐渐下降的，但在后面几次迭代时，loss逐渐上升。如第10次迭代和第20次迭代的loss是近似的。



**任务 2**：执行以下操作：

1. 按**重置**按钮。
2. 降低`学习速率`。
3. 多次按“步”按钮。

较低的学习速率对收敛有何影响？了解模型收敛所需的步数，并了解模型收敛的顺滑平稳程度。尝试较低的学习速率。能否发现因过慢而无用的学习速率？（您将在练习的正下方找到相关讨论。）

答：降低学习速率，loss会逐渐缓慢下降。

## 降低损失 (Reducing Loss)：检查您的理解情况

### 检查您的理解情况：批量大小

问题：基于大型数据集执行梯度下降法时，以下哪个批量大小可能比较高效？

A. 小批量或甚至包含一个样本的批量 (SGD)。

B. 全批量。

答：正确答案是A。小批量或甚至包含一个样本的批量 (SGD)。

令人惊讶的是，在小批量或甚至包含一个样本的批量上执行梯度下降法通常比全批量更高效。毕竟，计算一个样本的梯度要比计算数百万个样本的梯度成本低的多。为确保获得良好的代表性样本，该算法在每次迭代时都会抽取另一个随机小批量数据（或包含一个样本的批量数据）。

**关键字词**

[批量](https://developers.google.com/machine-learning/crash-course/glossary#batch)

[批量大小](https://developers.google.com/machine-learning/crash-course/glossary#batch_size)

[小批量](https://developers.google.com/machine-learning/crash-course/glossary#mini-batch)

[随机梯度下降法](https://developers.google.com/machine-learning/crash-course/glossary#stochastic_gradient_descent_(SGD))

# Reference

[降低损失](https://developers.google.com/machine-learning/crash-course/reducing-loss/video-lecture)





